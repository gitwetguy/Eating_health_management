{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80800 images belonging to 101 classes.\n",
      "Found 20199 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\"\"\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\"\"\"\n",
    "\n",
    "data_dir = 'images/'\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "## ***Data Collection***\n",
    "\n",
    "#datagen = ImageDataGenerator(rescale = 1./255, validation_split = 0.2)\n",
    "datagen = ImageDataGenerator(rescale=1. / 255, validation_split=0.2)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMG_W=250\n",
    "IMG_H=250\n",
    "\n",
    "train_data = datagen.flow_from_directory(data_dir, target_size = (IMG_W,IMG_H), batch_size = BATCH_SIZE, class_mode = 'categorical',\n",
    "                                        subset = 'training')\n",
    "\n",
    "val_data = datagen.flow_from_directory(data_dir, target_size = (IMG_W,IMG_H), batch_size = BATCH_SIZE, class_mode = 'categorical',\n",
    "                                        subset = 'validation')\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "ResNet_V2_50 = 'https://tfhub.dev/google/imagenet/resnet_v1_50/classification/5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-27037bf82dde6f2c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-27037bf82dde6f2c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6012;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ***ResNet Model Building***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.4301 - accuracy: 0.6300\n",
      "Epoch 00001: val_loss improved from inf to 1.77165, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 266s 205ms/step - loss: 1.4301 - accuracy: 0.6300 - val_loss: 1.7717 - val_accuracy: 0.5709 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.3808 - accuracy: 0.6394\n",
      "Epoch 00002: val_loss improved from 1.77165 to 1.76630, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 250s 198ms/step - loss: 1.3808 - accuracy: 0.6394 - val_loss: 1.7663 - val_accuracy: 0.5730 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.3435 - accuracy: 0.6481\n",
      "Epoch 00003: val_loss did not improve from 1.76630\n",
      "1263/1263 [==============================] - 255s 202ms/step - loss: 1.3435 - accuracy: 0.6481 - val_loss: 1.7877 - val_accuracy: 0.5763 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.3158 - accuracy: 0.6536\n",
      "Epoch 00004: val_loss did not improve from 1.76630\n",
      "1263/1263 [==============================] - 256s 203ms/step - loss: 1.3158 - accuracy: 0.6536 - val_loss: 1.7986 - val_accuracy: 0.5739 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.2899 - accuracy: 0.6589\n",
      "Epoch 00005: val_loss did not improve from 1.76630\n",
      "1263/1263 [==============================] - 251s 198ms/step - loss: 1.2899 - accuracy: 0.6589 - val_loss: 1.7834 - val_accuracy: 0.5820 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.2689 - accuracy: 0.6632\n",
      "Epoch 00006: val_loss did not improve from 1.76630\n",
      "1263/1263 [==============================] - 251s 199ms/step - loss: 1.2689 - accuracy: 0.6632 - val_loss: 1.8142 - val_accuracy: 0.5751 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.2480 - accuracy: 0.6700\n",
      "Epoch 00007: val_loss did not improve from 1.76630\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1263/1263 [==============================] - 255s 202ms/step - loss: 1.2480 - accuracy: 0.6700 - val_loss: 1.8301 - val_accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 1.0280 - accuracy: 0.7277\n",
      "Epoch 00008: val_loss improved from 1.76630 to 1.69294, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 251s 199ms/step - loss: 1.0280 - accuracy: 0.7277 - val_loss: 1.6929 - val_accuracy: 0.5980 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9942 - accuracy: 0.7384\n",
      "Epoch 00009: val_loss improved from 1.69294 to 1.67703, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 249s 197ms/step - loss: 0.9942 - accuracy: 0.7384 - val_loss: 1.6770 - val_accuracy: 0.5995 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9848 - accuracy: 0.7402\n",
      "Epoch 00010: val_loss improved from 1.67703 to 1.67566, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 250s 198ms/step - loss: 0.9848 - accuracy: 0.7402 - val_loss: 1.6757 - val_accuracy: 0.5984 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9800 - accuracy: 0.7433\n",
      "Epoch 00011: val_loss improved from 1.67566 to 1.67299, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 249s 197ms/step - loss: 0.9800 - accuracy: 0.7433 - val_loss: 1.6730 - val_accuracy: 0.5979 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9761 - accuracy: 0.7436\n",
      "Epoch 00012: val_loss improved from 1.67299 to 1.67273, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 251s 198ms/step - loss: 0.9761 - accuracy: 0.7436 - val_loss: 1.6727 - val_accuracy: 0.5993 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9736 - accuracy: 0.7447\n",
      "Epoch 00013: val_loss improved from 1.67273 to 1.67098, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 257s 203ms/step - loss: 0.9736 - accuracy: 0.7447 - val_loss: 1.6710 - val_accuracy: 0.5994 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9708 - accuracy: 0.7456\n",
      "Epoch 00014: val_loss did not improve from 1.67098\n",
      "1263/1263 [==============================] - 260s 206ms/step - loss: 0.9708 - accuracy: 0.7456 - val_loss: 1.6719 - val_accuracy: 0.5976 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9687 - accuracy: 0.7460\n",
      "Epoch 00015: val_loss improved from 1.67098 to 1.66938, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 261s 206ms/step - loss: 0.9687 - accuracy: 0.7460 - val_loss: 1.6694 - val_accuracy: 0.5979 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9662 - accuracy: 0.7457\n",
      "Epoch 00016: val_loss did not improve from 1.66938\n",
      "1263/1263 [==============================] - 253s 200ms/step - loss: 0.9662 - accuracy: 0.7457 - val_loss: 1.6709 - val_accuracy: 0.5983 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9648 - accuracy: 0.7472\n",
      "Epoch 00017: val_loss did not improve from 1.66938\n",
      "1263/1263 [==============================] - 239s 189ms/step - loss: 0.9648 - accuracy: 0.7472 - val_loss: 1.6729 - val_accuracy: 0.5973 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9630 - accuracy: 0.7471\n",
      "Epoch 00018: val_loss did not improve from 1.66938\n",
      "1263/1263 [==============================] - 241s 191ms/step - loss: 0.9630 - accuracy: 0.7471 - val_loss: 1.6723 - val_accuracy: 0.5975 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.7482\n",
      "Epoch 00019: val_loss did not improve from 1.66938\n",
      "1263/1263 [==============================] - 253s 200ms/step - loss: 0.9611 - accuracy: 0.7482 - val_loss: 1.6719 - val_accuracy: 0.5978 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9598 - accuracy: 0.7484\n",
      "Epoch 00020: val_loss did not improve from 1.66938\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1263/1263 [==============================] - 248s 196ms/step - loss: 0.9598 - accuracy: 0.7484 - val_loss: 1.6726 - val_accuracy: 0.5976 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9322 - accuracy: 0.7579\n",
      "Epoch 00021: val_loss improved from 1.66938 to 1.66698, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 260s 206ms/step - loss: 0.9322 - accuracy: 0.7579 - val_loss: 1.6670 - val_accuracy: 0.5982 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9306 - accuracy: 0.7586\n",
      "Epoch 00022: val_loss improved from 1.66698 to 1.66654, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 255s 202ms/step - loss: 0.9306 - accuracy: 0.7586 - val_loss: 1.6665 - val_accuracy: 0.5980 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9301 - accuracy: 0.7593\n",
      "Epoch 00023: val_loss improved from 1.66654 to 1.66634, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 258s 204ms/step - loss: 0.9301 - accuracy: 0.7593 - val_loss: 1.6663 - val_accuracy: 0.5979 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9298 - accuracy: 0.7585\n",
      "Epoch 00024: val_loss did not improve from 1.66634\n",
      "1263/1263 [==============================] - 254s 201ms/step - loss: 0.9298 - accuracy: 0.7585 - val_loss: 1.6664 - val_accuracy: 0.5980 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9296 - accuracy: 0.7587\n",
      "Epoch 00025: val_loss improved from 1.66634 to 1.66617, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 262s 207ms/step - loss: 0.9296 - accuracy: 0.7587 - val_loss: 1.6662 - val_accuracy: 0.5981 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9293 - accuracy: 0.7588\n",
      "Epoch 00026: val_loss improved from 1.66617 to 1.66598, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1263/1263 [==============================] - 258s 204ms/step - loss: 0.9293 - accuracy: 0.7588 - val_loss: 1.6660 - val_accuracy: 0.5985 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9261 - accuracy: 0.7601\n",
      "Epoch 00027: val_loss improved from 1.66598 to 1.66594, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 254s 201ms/step - loss: 0.9261 - accuracy: 0.7601 - val_loss: 1.6659 - val_accuracy: 0.5985 - lr: 1.0000e-06\n",
      "Epoch 28/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.7600\n",
      "Epoch 00028: val_loss improved from 1.66594 to 1.66591, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 262s 207ms/step - loss: 0.9260 - accuracy: 0.7600 - val_loss: 1.6659 - val_accuracy: 0.5983 - lr: 1.0000e-06\n",
      "Epoch 29/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9259 - accuracy: 0.7600\n",
      "Epoch 00029: val_loss improved from 1.66591 to 1.66590, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 253s 200ms/step - loss: 0.9259 - accuracy: 0.7600 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-06\n",
      "Epoch 30/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9259 - accuracy: 0.7599\n",
      "Epoch 00030: val_loss improved from 1.66590 to 1.66590, saving model to logs\\20220122-022601_Resnet_V2_50_best_weight.hdf5\n",
      "1263/1263 [==============================] - 257s 203ms/step - loss: 0.9259 - accuracy: 0.7599 - val_loss: 1.6659 - val_accuracy: 0.5983 - lr: 1.0000e-06\n",
      "Epoch 31/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.7600\n",
      "Epoch 00031: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 262s 207ms/step - loss: 0.9258 - accuracy: 0.7600 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-06\n",
      "Epoch 32/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.7602\n",
      "Epoch 00032: val_loss did not improve from 1.66590\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1263/1263 [==============================] - 249s 197ms/step - loss: 0.9258 - accuracy: 0.7602 - val_loss: 1.6659 - val_accuracy: 0.5981 - lr: 1.0000e-06\n",
      "Epoch 33/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7600\n",
      "Epoch 00033: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 259s 205ms/step - loss: 0.9254 - accuracy: 0.7600 - val_loss: 1.6659 - val_accuracy: 0.5981 - lr: 1.0000e-07\n",
      "Epoch 34/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7601\n",
      "Epoch 00034: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 813s 644ms/step - loss: 0.9254 - accuracy: 0.7601 - val_loss: 1.6659 - val_accuracy: 0.5981 - lr: 1.0000e-07\n",
      "Epoch 35/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7601\n",
      "Epoch 00035: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 259s 205ms/step - loss: 0.9254 - accuracy: 0.7601 - val_loss: 1.6659 - val_accuracy: 0.5981 - lr: 1.0000e-07\n",
      "Epoch 36/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7600\n",
      "Epoch 00036: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 257s 203ms/step - loss: 0.9254 - accuracy: 0.7600 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-07\n",
      "Epoch 37/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7600\n",
      "Epoch 00037: val_loss did not improve from 1.66590\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1263/1263 [==============================] - 260s 206ms/step - loss: 0.9254 - accuracy: 0.7600 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-07\n",
      "Epoch 38/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7601\n",
      "Epoch 00038: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 598s 473ms/step - loss: 0.9254 - accuracy: 0.7601 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-08\n",
      "Epoch 39/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7601\n",
      "Epoch 00039: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 2277s 2s/step - loss: 0.9254 - accuracy: 0.7601 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-08\n",
      "Epoch 40/50\n",
      "1263/1263 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7601\n",
      "Epoch 00040: val_loss did not improve from 1.66590\n",
      "1263/1263 [==============================] - 261s 207ms/step - loss: 0.9254 - accuracy: 0.7601 - val_loss: 1.6659 - val_accuracy: 0.5982 - lr: 1.0000e-08\n",
      "Epoch 00040: early stopping\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 1.6659 - accuracy: 0.5982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.665911078453064, 0.5981979370117188]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ResNet = tf.keras.Sequential([\n",
    "    hub.KerasLayer(ResNet_V2_50, trainable = False, input_shape = (250,250,3), name = 'Resnet_V2_50'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(101, activation = 'softmax', name = 'Output_layer')\n",
    "])\n",
    "\n",
    "model_ResNet.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "#define callback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import datetime,os\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#1.ModelCheckpoint\n",
    "filepath = os.path.join(\"logs\", \"{}_Resnet_V2_50_best_weight.hdf5\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "mc = ModelCheckpoint(\n",
    "    filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "#2.EarlyStopping\n",
    "es=EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "#3.ReduceLROnPlateau\n",
    "rp=ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.001,\n",
    "    cooldown=0,\n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "model_ResNet.load_weights(\"./logs/20220122-015508_Resnet_V2_50_best_weight.hdf5.\")\n",
    "model_ResNet_history = model_ResNet.fit(train_data, epochs = 50, verbose = 1,validation_data=val_data,callbacks=[mc,rp,tb,es])\n",
    "\n",
    "model_ResNet.save(\"Resnet_V2_50_final_{}.h5\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "model_ResNet.save_weights('Resnet_V2_50_final_weight_{}.h5'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "model_ResNet.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Efficientnet_B0 Model Building***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Resnet_V2_50 (KerasLayer)   (None, 1000)              5330564   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1000)              0         \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 101)               101101    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,431,665\n",
      "Trainable params: 101,101\n",
      "Non-trainable params: 5,330,564\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_Efficientnet = tf.keras.Sequential([\n",
    "    hub.KerasLayer(Efficientnet_b0, trainable = False, input_shape = (250,250,3), name = 'Resnet_V2_50'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(101, activation = 'softmax', name = 'Output_layer')\n",
    "])\n",
    "\n",
    "model_Efficientnet.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_Efficientnet.summary()\n",
    "\n",
    "\n",
    "#define callback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import datetime,os\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#1.ModelCheckpoint\n",
    "filepath = os.path.join(\"logs\", \"weight.hdf5\")\n",
    "mc = ModelCheckpoint(\n",
    "    filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "#2.EarlyStopping\n",
    "#es=EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "\n",
    "#3.ReduceLROnPlateau\n",
    "rp=ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.001,\n",
    "    cooldown=0,\n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "\n",
    "efficientnet_model = model_Efficientnet.fit(train_data, epochs = 100, verbose = 1,validation_data=val_data,callbacks=[mc,rp,tb])\n",
    "\n",
    "model_Efficientnet.save(\"Efficientnet_b0_{}.h5\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "model_Efficientnet.save_weights('Efficientnet_b0_weight_{}.h5'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "model_Efficientnet.evaluate(val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Comparison Between Both Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_graph(history, history_1):\n",
    "    loss_res = history.history['loss']\n",
    "    loss_ef = history_1.history['loss']\n",
    "    \n",
    "    Accuracy_res = history.history['accuracy']\n",
    "    Accuracy_ef = history_1.history['accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    plt.plot(epochs, loss_res, label = 'ResNet Loss')\n",
    "    plt.plot(epochs, loss_ef, label = 'Efficientnet Loss')\n",
    "    plt.title('Epochs Vs Loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, Accuracy_res, label = 'ResNet Accuracy')\n",
    "    plt.plot(epochs, Accuracy_ef, label = 'Efficientnet Accuracy')\n",
    "    plt.title('Epochs Vs Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "plot_graph(resnet_model, efficientnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Efficientnet_B7 Model Building***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_Efficientnet_b7 = tf.keras.Sequential([\n",
    "    hub.KerasLayer(Efficientnet_b7, trainable = False, input_shape = (250,250,3), name = 'Efficientnet_b7'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(101, activation = 'softmax', name = 'Output_layer')\n",
    "])\n",
    "\n",
    "model_Efficientnet_b7.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_Efficientnet_b7.summary()\n",
    "\n",
    "\n",
    "#define callback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import datetime,os\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#1.ModelCheckpoint\n",
    "filepath = os.path.join(\"logs\", \"{}_best_weight.hdf5\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "mc = ModelCheckpoint(\n",
    "    filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "#2.EarlyStopping\n",
    "es=EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "#3.ReduceLROnPlateau\n",
    "rp=ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.001,\n",
    "    cooldown=0,\n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "\n",
    "efficientnet_b7_model = model_Efficientnet_b7.fit(train_data, epochs = 50, verbose = 1,validation_data=val_data,callbacks=[mc,rp,tb,es])\n",
    "\n",
    "model_Efficientnet_b7.save(\"Efficientnet_b7_final_{}.h5\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "model_Efficientnet_b7.save_weights('Efficientnet_b7_final_weight_{}.h5'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "model_Efficientnet_b7.evaluate(val_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
